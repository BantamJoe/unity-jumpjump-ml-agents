{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ML Agents\n",
    "## Proximal Policy Optimization (PPO)\n",
    "Contains an implementation of PPO as described [here](https://arxiv.org/abs/1707.06347)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from ppo.history import *\n",
    "from ppo.models import *\n",
    "from ppo.trainer import Trainer\n",
    "from unityagents import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### General parameters\n",
    "max_steps = 1e7 # Set maximum number of steps to run environment.\n",
    "run_path = \"jump2\" # The sub-directory name for model and summary statistics\n",
    "load_model = True # Whether to load a saved model.\n",
    "train_model = True # Whether to train the model.\n",
    "summary_freq = 10000 # Frequency at which to save training statistics.\n",
    "save_freq = 10000 # Frequency at which to save model.\n",
    "env_name = \"build2/jump.exe\" # Name of the training environment file.\n",
    "curriculum_file = \"jump2.json\"\n",
    "\n",
    "### Algorithm-specific parameters for tuning\n",
    "gamma = 0.99 # Reward discount rate.\n",
    "lambd = 0.95 # Lambda parameter for GAE.\n",
    "time_horizon = 2048 # How many steps to collect per agent before adding to buffer.\n",
    "beta = 1e-3 # Strength of entropy regularization\n",
    "num_epoch = 5 # Number of gradient descent steps per batch of experiences.\n",
    "num_layers = 1 # Number of hidden layers between state/observation encoding and value/policy layers.\n",
    "epsilon = 0.2 # Acceptable threshold around ratio of old and new policy probabilities.\n",
    "buffer_size = 2048 # How large the experience buffer should be before gradient descent.\n",
    "learning_rate = 3e-4 # Model learning rate.\n",
    "hidden_units = 32 # Number of units in hidden layer.\n",
    "batch_size = 512 # How many experiences per gradient descent update step.\n",
    "normalize = True\n",
    "\n",
    "### Logging dictionary for hyperparameters\n",
    "hyperparameter_dict = {'max_steps':max_steps, 'run_path':run_path, 'env_name':env_name,\n",
    "    'curriculum_file':curriculum_file, 'gamma':gamma, 'lambd':lambd, 'time_horizon':time_horizon,\n",
    "    'beta':beta, 'num_epoch':num_epoch, 'epsilon':epsilon, 'buffe_size':buffer_size,\n",
    "    'leaning_rate':learning_rate, 'hidden_units':hidden_units, 'batch_size':batch_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'BottleFlipAcademy' started successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: BottleFlipAcademy\n",
      "        Number of brains: 1\n",
      "        Reset Parameters :\n",
      "\t\trandom_direction -> 1.0\n",
      "\t\tmin_scale -> 0.5\n",
      "\t\tmax_distance -> 3.0\n",
      "Unity brain name: BottleFlipBrain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 2\n",
      "        Action space type: continuous\n",
      "        Action space size (per agent): 1\n",
      "        Memory space size (per agent): 0\n",
      "        Action descriptions: \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name, curriculum=curriculum_file)\n",
    "print(str(env))\n",
    "brain_name = env.external_brain_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "INFO:tensorflow:Restoring parameters from ./models/jump2\\model-1140000.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/jump2\\model-1140000.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n",
      "Step: 1150000. Mean Reward: -1.1274067607546439. Std of Reward: 0.7140787458563955.\n",
      "Saved Model\n",
      "Step: 1160000. Mean Reward: -1.1737106017191976. Std of Reward: 0.6603585639355769.\n",
      "Saved Model\n",
      "Step: 1170000. Mean Reward: -1.110317900378893. Std of Reward: 0.49730658029508906.\n",
      "Saved Model\n",
      "Step: 1180000. Mean Reward: -1.1309230411532203. Std of Reward: 0.5367081359857945.\n",
      "Saved Model\n",
      "Step: 1190000. Mean Reward: -1.116686114352392. Std of Reward: 0.5397300294990185.\n",
      "Saved Model\n",
      "Step: 1200000. Mean Reward: -1.0. Std of Reward: 0.0.\n",
      "Saved Model\n",
      "Step: 1210000. Mean Reward: -1.0. Std of Reward: 0.0.\n",
      "Saved Model\n",
      "Step: 1220000. Mean Reward: -0.9997777185027118. Std of Reward: 0.025822375622712574.\n",
      "Saved Model\n",
      "Step: 1230000. Mean Reward: -1.0141696791181327. Std of Reward: 0.18921837602867425.\n",
      "Saved Model\n",
      "Step: 1240000. Mean Reward: -1.0536003600360035. Std of Reward: 0.3477885627681129.\n",
      "Saved Model\n",
      "Step: 1250000. Mean Reward: -1.0574351793296595. Std of Reward: 0.3785406625748665.\n",
      "Saved Model\n",
      "Step: 1260000. Mean Reward: -1.1110514747226907. Std of Reward: 1.0961656058659948.\n",
      "Saved Model\n",
      "Step: 1270000. Mean Reward: -0.9831007478066301. Std of Reward: 1.9295220660366066.\n",
      "Saved Model\n",
      "Step: 1280000. Mean Reward: -0.3695045695045695. Std of Reward: 3.8853176710974715.\n",
      "Saved Model\n",
      "Step: 1290000. Mean Reward: 1.0698003180773987. Std of Reward: 6.573169785650429.\n",
      "Saved Model\n",
      "Step: 1300000. Mean Reward: 2.0525045955882355. Std of Reward: 8.165119454869908.\n",
      "Saved Model\n",
      "Step: 1310000. Mean Reward: 2.3191672825819167. Std of Reward: 8.619609995147357.\n",
      "Saved Model\n",
      "Step: 1320000. Mean Reward: 4.070151770657673. Std of Reward: 9.883732328045225.\n",
      "Saved Model\n",
      "Step: 1330000. Mean Reward: 4.888094317742559. Std of Reward: 10.77841716115133.\n",
      "Saved Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 1 : \trandom_direction -> 0, min_scale -> 0.9, max_distance -> 1.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1340000. Mean Reward: -0.12386107352277853. Std of Reward: 2.5861335020445533.\n",
      "Saved Model\n",
      "Step: 1350000. Mean Reward: 0.550770218228498. Std of Reward: 3.8303927874464825.\n",
      "Saved Model\n",
      "Step: 1360000. Mean Reward: 1.0501820484407156. Std of Reward: 5.126106289793513.\n",
      "Saved Model\n",
      "Step: 1370000. Mean Reward: 2.0794156414762743. Std of Reward: 6.635808178795247.\n",
      "Saved Model\n",
      "Step: 1380000. Mean Reward: 3.181113662456946. Std of Reward: 7.303898532768478.\n",
      "Saved Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 2 : \trandom_direction -> 0, min_scale -> 0.7, max_distance -> 1.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1390000. Mean Reward: -0.9381179595651532. Std of Reward: 0.9309146128864142.\n",
      "Saved Model\n",
      "Step: 1400000. Mean Reward: -0.9880642874025053. Std of Reward: 0.7040832466925914.\n",
      "Saved Model\n",
      "Step: 1410000. Mean Reward: -0.7703986429177269. Std of Reward: 0.8660157302835358.\n",
      "Saved Model\n",
      "Step: 1420000. Mean Reward: -0.7528381259724544. Std of Reward: 0.9256153784726607.\n",
      "Saved Model\n",
      "Step: 1430000. Mean Reward: 0.07509215376513954. Std of Reward: 3.5626856064375905.\n",
      "Saved Model\n",
      "Step: 1440000. Mean Reward: 3.50276837896032. Std of Reward: 8.091515070994863.\n",
      "Saved Model\n",
      "Step: 1450000. Mean Reward: 3.1077441077441077. Std of Reward: 8.381144432838745.\n",
      "Saved Model\n",
      "Step: 1460000. Mean Reward: 2.917728381675251. Std of Reward: 6.272625743026066.\n",
      "Saved Model\n",
      "Step: 1470000. Mean Reward: 4.471507352941177. Std of Reward: 7.867726375270203.\n",
      "Saved Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 3 : \trandom_direction -> 0, min_scale -> 0.5, max_distance -> 1.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1480000. Mean Reward: 3.4992262457443517. Std of Reward: 6.672890503925557.\n",
      "Saved Model\n",
      "Step: 1490000. Mean Reward: 4.303851640513552. Std of Reward: 7.314713602974397.\n",
      "Saved Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 4 : \trandom_direction -> 0, min_scale -> 1, max_distance -> 1.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1500000. Mean Reward: 5.606114311032344. Std of Reward: 8.380088951706055.\n",
      "Saved Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 5 : \trandom_direction -> 0, min_scale -> 0.9, max_distance -> 1.5\n",
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 6 : \trandom_direction -> 0, min_scale -> 0.7, max_distance -> 1.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1510000. Mean Reward: 5.062684055532183. Std of Reward: 10.260001535092231.\n",
      "Saved Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 7 : \trandom_direction -> 0, min_scale -> 0.5, max_distance -> 1.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1520000. Mean Reward: 5.839075035394054. Std of Reward: 8.694357214581064.\n",
      "Saved Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 8 : \trandom_direction -> 0, min_scale -> 1, max_distance -> 2\n",
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 9 : \trandom_direction -> 0, min_scale -> 0.9, max_distance -> 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1530000. Mean Reward: 12.23018018018018. Std of Reward: 10.350128996191563.\n",
      "Saved Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 10 : \trandom_direction -> 0, min_scale -> 0.7, max_distance -> 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1540000. Mean Reward: 0.4590474944393563. Std of Reward: 2.7146725026261262.\n",
      "Saved Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 11 : \trandom_direction -> 0, min_scale -> 0.5, max_distance -> 2\n",
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 12 : \trandom_direction -> 0, min_scale -> 1, max_distance -> 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1550000. Mean Reward: 5.906217345872518. Std of Reward: 7.796219779438782.\n",
      "Saved Model\n",
      "Step: 1560000. Mean Reward: -0.9267928858290304. Std of Reward: 0.9598535615242207.\n",
      "Saved Model\n",
      "Step: 1570000. Mean Reward: -0.9483186700491175. Std of Reward: 0.747885781557409.\n",
      "Saved Model\n",
      "Step: 1580000. Mean Reward: 2.067628749292586. Std of Reward: 5.644368477432351.\n",
      "Saved Model\n",
      "Step: 1590000. Mean Reward: 3.554904259476358. Std of Reward: 6.423917791619926.\n",
      "Saved Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 13 : \trandom_direction -> 0, min_scale -> 0.9, max_distance -> 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1600000. Mean Reward: 3.5201342281879193. Std of Reward: 6.112883146999933.\n",
      "Saved Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 14 : \trandom_direction -> 0, min_scale -> 0.7, max_distance -> 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1610000. Mean Reward: 0.02213719595517901. Std of Reward: 3.064497838775721.\n",
      "Saved Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 15 : \trandom_direction -> 0, min_scale -> 0.5, max_distance -> 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1620000. Mean Reward: 4.506346967559944. Std of Reward: 6.880833484979833.\n",
      "Saved Model\n",
      "Step: 1630000. Mean Reward: 2.3353696741854635. Std of Reward: 5.657123859753659.\n",
      "Saved Model\n",
      "Step: 1640000. Mean Reward: 4.297592510031208. Std of Reward: 5.957405915562527.\n",
      "Saved Model\n",
      "Step: 1650000. Mean Reward: 1.900997697620875. Std of Reward: 3.9233897309678563.\n",
      "Saved Model\n",
      "Step: 1660000. Mean Reward: 4.770833333333333. Std of Reward: 6.576968178170445.\n",
      "Saved Model\n",
      "Step: 1670000. Mean Reward: 3.5770758839888757. Std of Reward: 5.7010007303956955.\n",
      "Saved Model\n",
      "Step: 1680000. Mean Reward: 0.6416555151978589. Std of Reward: 4.368800099736575.\n",
      "Saved Model\n",
      "Step: 1690000. Mean Reward: 1.1642456941274124. Std of Reward: 3.0553023227291525.\n",
      "Saved Model\n",
      "Step: 1700000. Mean Reward: 1.3160203139427515. Std of Reward: 4.465132905404935.\n",
      "Saved Model\n",
      "Step: 1710000. Mean Reward: 2.0686137506987143. Std of Reward: 5.367643817030653.\n",
      "Saved Model\n",
      "Step: 1720000. Mean Reward: 2.6483773837403812. Std of Reward: 5.773868076906962.\n",
      "Saved Model\n",
      "Step: 1730000. Mean Reward: 3.7827715355805243. Std of Reward: 5.872912162695241.\n",
      "Saved Model\n",
      "Step: 1740000. Mean Reward: 2.6198763825634352. Std of Reward: 5.0085345363714096.\n",
      "Saved Model\n",
      "Step: 1750000. Mean Reward: 2.0466702326825352. Std of Reward: 4.07544870816449.\n",
      "Saved Model\n",
      "Step: 1760000. Mean Reward: 5.290765171503958. Std of Reward: 6.7503850526546705.\n",
      "Saved Model\n",
      "Step: 1770000. Mean Reward: 0.7496532593619972. Std of Reward: 2.8457665989684076.\n",
      "Saved Model\n",
      "Step: 1780000. Mean Reward: 4.846267918932279. Std of Reward: 7.297359313644964.\n",
      "Saved Model\n",
      "Step: 1790000. Mean Reward: 4.295433996383363. Std of Reward: 6.041754348474223.\n",
      "Saved Model\n",
      "Step: 1800000. Mean Reward: -0.5444384072032912. Std of Reward: 1.626769697443204.\n",
      "Saved Model\n",
      "Step: 1810000. Mean Reward: 0.7066150479717219. Std of Reward: 3.535834542162051.\n",
      "Saved Model\n",
      "Step: 1820000. Mean Reward: 4.0778110303548525. Std of Reward: 5.922105110581553.\n",
      "Saved Model\n",
      "Step: 1830000. Mean Reward: 4.151197604790419. Std of Reward: 6.128325519476896.\n",
      "Saved Model\n",
      "Step: 1840000. Mean Reward: 4.1212719298245615. Std of Reward: 6.177144428817835.\n",
      "Saved Model\n",
      "Step: 1850000. Mean Reward: 4.639470013947001. Std of Reward: 6.34902844915433.\n",
      "Saved Model\n",
      "Step: 1860000. Mean Reward: 3.859375. Std of Reward: 6.05056496961071.\n",
      "Saved Model\n",
      "Step: 1870000. Mean Reward: 2.609213917525773. Std of Reward: 5.195624661115134.\n",
      "Saved Model\n",
      "Step: 1880000. Mean Reward: 4.197577092511013. Std of Reward: 6.406705486190857.\n",
      "Saved Model\n",
      "Step: 1890000. Mean Reward: 3.857952246054229. Std of Reward: 5.583048766062056.\n",
      "Saved Model\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if curriculum_file == \"None\":\n",
    "    curriculum_file = None\n",
    "\n",
    "\n",
    "def get_progress():\n",
    "    if curriculum_file is not None:\n",
    "        if env._curriculum.measure_type == \"progress\":\n",
    "            return steps / max_steps\n",
    "        elif env._curriculum.measure_type == \"reward\":\n",
    "            return last_reward\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create the Tensorflow model graph\n",
    "ppo_model = create_agent_model(env, lr=learning_rate,\n",
    "                               h_size=hidden_units, epsilon=epsilon,\n",
    "                               beta=beta, max_step=max_steps, \n",
    "                               normalize=normalize, num_layers=num_layers)\n",
    "\n",
    "is_continuous = (env.brains[brain_name].action_space_type == \"continuous\")\n",
    "use_observations = (env.brains[brain_name].number_observations > 0)\n",
    "use_states = (env.brains[brain_name].state_space_size > 0)\n",
    "\n",
    "model_path = './models/{}'.format(run_path)\n",
    "summary_path = './summaries/{}'.format(run_path)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "if not os.path.exists(summary_path):\n",
    "    os.makedirs(summary_path)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Instantiate model parameters\n",
    "    if load_model:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    steps, last_reward = sess.run([ppo_model.global_step, ppo_model.last_reward])    \n",
    "    summary_writer = tf.summary.FileWriter(summary_path)\n",
    "    info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "    trainer = Trainer(ppo_model, sess, info, is_continuous, use_observations, use_states, train_model)\n",
    "    if train_model:\n",
    "        trainer.write_text(summary_writer, 'Hyperparameters', hyperparameter_dict, steps)\n",
    "    while steps <= max_steps:\n",
    "        if env.global_done:\n",
    "            info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "        # Decide and take an action\n",
    "        new_info = trainer.take_action(info, env, brain_name, steps, normalize)\n",
    "        info = new_info\n",
    "        trainer.process_experiences(info, time_horizon, gamma, lambd)\n",
    "        if len(trainer.training_buffer['actions']) > buffer_size and train_model:\n",
    "            # Perform gradient descent with experience buffer\n",
    "            trainer.update_model(batch_size, num_epoch)\n",
    "        if steps % summary_freq == 0 and steps != 0 and train_model:\n",
    "            # Write training statistics to tensorboard.\n",
    "            trainer.write_summary(summary_writer, steps, env._curriculum.lesson_number)\n",
    "        if steps % save_freq == 0 and steps != 0 and train_model:\n",
    "            # Save Tensorflow model\n",
    "            save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "        steps += 1\n",
    "        sess.run(ppo_model.increment_step)\n",
    "        if len(trainer.stats['cumulative_reward']) > 0:\n",
    "            mean_reward = np.mean(trainer.stats['cumulative_reward'])\n",
    "            sess.run(ppo_model.update_reward, feed_dict={ppo_model.new_reward: mean_reward})\n",
    "            last_reward = sess.run(ppo_model.last_reward)\n",
    "    # Final save Tensorflow model\n",
    "    if steps != 0 and train_model:\n",
    "        save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "env.close()\n",
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the trained Tensorflow graph\n",
    "Once the model has been trained and saved, we can export it as a .bytes file which Unity can embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 32\n",
      "INFO:tensorflow:Restoring parameters from ./models/jump2\\model-1140000.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/jump2\\model-1140000.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 8 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 8 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 8 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
